{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "petfinder_com_adoption_speed_prediction cat/dog.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busyML/Petfinder.com-Adoption-Time-Predictor/blob/master/petfinder_com_adoption_speed_prediction_cat_dog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "mzQcF-MuTAn_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd #Pandas is the most popular library for manipulating data. Think of it as an Excel but a million times faster and more practical.\n",
        "import numpy as np # This library allows to easily carry out simple and complex mathematical operations.\n",
        "import matplotlib.pyplot as plt #Allows us to plot data, create graphs and visualize data. Perfect for your Powerpoint slides ;)\n",
        "import sklearn #The one and only. This amazing library holds all the secrets. Containing powerful algorithms packed in a single line of code, this is where the magic will happen.\n",
        "import sklearn.model_selection # more of sklearn. It is a big library, but trust me it is worth it.\n",
        "import sklearn.preprocessing \n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, explained_variance_score,mean_absolute_error,mean_squared_error,precision_score,recall_score, accuracy_score,f1_score\n",
        "from sklearn.utils import shuffle\n",
        "import os\n",
        "\n",
        "\n",
        "import random # Allows us to call random numbers, occasionally very useful.\n",
        "import pprint#Allows us to neatly display text\n",
        "from collections import OrderedDict\n",
        "\n",
        "import seaborn as sns\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false,
        "id": "HvESaDdxTAoO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Introduction\n",
        "Hi everyone, I hope the title of this Kernel made caused you to clickbait here. The main discussion point of this kernel are Feature creating and Model speration. These allowed me to get slightly better results and perhaps someone with more experience ( i am but a beginner) would be able to make better use of these"
      ]
    },
    {
      "metadata": {
        "_uuid": "7c5ae3a39e8f5cff49804915408315d0be70b5f1",
        "id": "u1D5uzVlTAoR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I'm importing the Quadratic Weighted Kappa function that [Aman Arora was so kind to share for all of us](https://www.kaggle.com/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps):"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3eedbb86bf2029d472f06321998d786430ffa596",
        "id": "xXWsnn16TAoT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.scorer import make_scorer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def quadratic_kappa(y_true, y_pred):\n",
        "    \"\"\"This function calculates the Quadratic Kappa Metric used for Evaluation in the PetFinder competition\n",
        "    at Kaggle. It returns the Quadratic Weighted Kappa metric score between the actual and the predicted values \n",
        "    of adoption rating.\"\"\"\n",
        "    w = np.zeros((5,5))\n",
        "    O = confusion_matrix(y_true, y_pred)\n",
        "    for i in range(len(w)): \n",
        "        for j in range(len(w)):\n",
        "            w[i][j] = float(((i-j)**2)/(5-1)**2)\n",
        "    \n",
        "    act_hist=np.zeros([5])\n",
        "    for item in y_true: \n",
        "        act_hist[item]+=1\n",
        "    \n",
        "    pred_hist=np.zeros([5])\n",
        "    for item in y_pred: \n",
        "        pred_hist[item]+=1\n",
        "                         \n",
        "    E = np.outer(act_hist, pred_hist);\n",
        "    E = E/E.sum();\n",
        "    O = O/O.sum();\n",
        "    \n",
        "    num=0\n",
        "    den=0\n",
        "    for i in range(len(w)):\n",
        "        for j in range(len(w)):\n",
        "            num+=w[i][j]*O[i][j]\n",
        "            den+=w[i][j]*E[i][j]\n",
        "    return (1 - (num/den))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2345bcad219c32953b22590857c2eace6fe03570",
        "id": "h9xmDFSmTAob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I always found this awesome correlation matrix function provided by [Enrique Herroreos in his kernel](https://www.kaggle.com/kikexclusive/curiosity-didn-t-kill-the-cat-all-in-one). Big up!"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f1c37d33782b89229f3c2a9f8ecb8683ebadc15",
        "id": "RkL9Y7NsTAod",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_correlation_matrix(df):\n",
        "    corr = df.corr()\n",
        "\n",
        "    # Generate a mask for the upper triangle\n",
        "    mask = np.zeros_like(corr, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    f, ax = plt.subplots(figsize=(20, 9))\n",
        "\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a9cf288dbf80adadc3d68cc8a0a5e6c3575549e9",
        "id": "r7xpaW2rTAok",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part I - Data Cleaning"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4165d6fb9e414fccca9f1a46c3e71c0f5c57d3c8",
        "id": "bEO_5L9iTAom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's load our data\n",
        "print(os.listdir(\"../input\"))\n",
        "\n",
        "data = pd.read_csv(\"../input/train/train.csv\")\n",
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a143b8a5d33ee6a2febc07b5efe50d41a8a7eadf",
        "id": "O0ZCnXmaTAos",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1 - Data Dropping"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dbd2c9cff6957523da4ae21923bffa5cb50fe1d",
        "id": "h1IBOBZpTAou",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.set_index('PetID', inplace=True)\n",
        "#I'm not going to use NLP in this Kernel, soz!\n",
        "data.drop(columns=['RescuerID','Name','Description'], inplace=True)\n",
        "\n",
        "#Shuffling data... Just in case\n",
        "data= data.sample(frac=1, random_state=85)\n",
        "\n",
        "data.head() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a70a001f30a4107c9c4087818ab46d1902fc0bcf",
        "id": "YqanuglPTAo1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Feature Creation\n",
        "\n",
        "So here is where the fun begins. There were some features I weren't too happy with, namely the \"States\", \"Breeds\" and \"Colours\".\n",
        "\n",
        "For the Breeds an States, I though One-Hot-Encoding would be the best idea because I dataset is relatively small. So what I wanted to find was data that would be numerical and hierchacical to substitute with. The question became, what information does the State and Breed provide us with?\n",
        "\n",
        "* **For the States** the following two properties would have a direct impact on the Adoption Speed : Population Size and Internet Penetration . So I added them to the state_labels.csv. ( I couldn't find a dataset on the internet penetration rate per state so I used urbanization rate as the closest proxy). You can find t[he complete database here](https://raw.githubusercontent.com/busyML/Petfinder.com-Adoption-Time-Predictor/master/state_labels.csv)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4da1d1b3594fd96874f9c35af581645c9aa6c1e",
        "id": "soYK_71rTAo3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "states = pd.read_csv('../input/state_labels.csv')\n",
        "\n",
        "\n",
        "states.at[0, \"State Urbanisation\"] = 71.9\n",
        "states.at[1, \"State Urbanisation\"] = 64.6\n",
        "states.at[2, \"State Urbanisation\"] = 42.4\n",
        "states.at[3, \"State Urbanisation\"] = 100\n",
        "states.at[4, \"State Urbanisation\"] = 82.3\n",
        "states.at[5, \"State Urbanisation\"] = 86.5\n",
        "states.at[6, \"State Urbanisation\"] = 66.5\n",
        "states.at[7, \"State Urbanisation\"] = 50.5\n",
        "states.at[8, \"State Urbanisation\"] = 69.7\n",
        "states.at[9, \"State Urbanisation\"] = 51.4\n",
        "states.at[10, \"State Urbanisation\"] = 90.8\n",
        "states.at[11, \"State Urbanisation\"] = 54\n",
        "states.at[12, \"State Urbanisation\"] = 53.8\n",
        "states.at[13, \"State Urbanisation\"] = 91.4\n",
        "states.at[14, \"State Urbanisation\"] = 59.1\n",
        "\n",
        "states.at[0, \"State Population\"] = 3348283\n",
        "states.at[1, \"State Population\"] = 1890098\n",
        "states.at[2, \"State Population\"] = 1459994\n",
        "states.at[3, \"State Population\"] = 1627172\n",
        "states.at[4, \"State Population\"] = 86908\n",
        "states.at[5, \"State Population\"] = 788706\n",
        "states.at[6, \"State Population\"] = 997071\n",
        "states.at[7, \"State Population\"] = 1443365\n",
        "states.at[8, \"State Population\"] = 2258428\n",
        "states.at[9, \"State Population\"] = 227025\n",
        "states.at[10, \"State Population\"] = 1520143\n",
        "states.at[11, \"State Population\"] = 3117405\n",
        "states.at[12, \"State Population\"] = 72420009\n",
        "states.at[13, \"State Population\"] = 5411324\n",
        "states.at[14, \"State Population\"] = 1015776.9\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d0594297724fdd60088a32ffca3d8c2c9e68c21",
        "id": "f1wYEmB0TApC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We convert the label dataframe into a dictionary that we then map onto our train dataframe:\n",
        "states_ubran_dict=states.set_index('StateID')['State Urbanisation'].to_dict()\n",
        "states_pop_dict=states.set_index('StateID')['State Population'].to_dict()\n",
        "\n",
        "data[\"State Urbanisation\"]=data[\"State\"].map(states_ubran_dict)\n",
        "data[\"State Population\"]=data[\"State\"].map(states_pop_dict)\n",
        "data.drop(columns=\"State\", inplace= True)\n",
        "\n",
        "\n",
        "data.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "472ab5bbccd9fe6988b01232bcffa9fbbcf3b3d1",
        "id": "kBFGxk-1TApO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "_uuid": "fee115f04b14cbc7c992189f70f880234696f5f9",
        "id": "zdYS4SLuTApQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I had a similar feeling about the breed feature, I wanted a way to be able to rank each breed. So I took the breed_labels file and added to it a ranked popularity based on [this dataset of breed popularity ](https://www.akc.org/expert-advice/news/most-popular-dog-breeds-full-ranking-list/). Here, the smaller the number, the greater the popularity."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8fa47b7fbb5820d80793da667875e53aa1af5374",
        "id": "T1st_Y0vTApT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "breeds=states = pd.read_csv('../input/breed_labels.csv')\n",
        "\n",
        "breeds.at[0,\"Ranked Popularity\"]= 147\n",
        "#Let's speed this up. Here is a list of the ranked popularity with the same index as the breed labels.\n",
        "breeds_popularity_list=[147,93,55,190,47,59,5,118,126,190,148,86,190,56,190,17,137,84,39,6,125,141,151,120,190,44,190,25,46,130,1,190,116,50,190,132,124,190,38,92,102,21,85,11,98,127,26,94,60,51,69,175,37,190,190,190,24,19,43,32,79,190,179,1,76,184,140,190,29,40,130,15,81,171,13,63,182,16,190,67,190,5,52,178,113,95,70,27,134,152,118,190,145,190,173,177,96,99,187,4,156,136,2,10,190,62,80,158,3,104,14,66,75,156,183,23,160,190,12,165,189,72,114,164,73,74,111,78,119,190,190,190,190,87,129,190,190,163,157,190,1,138,190,100,77,169,33,70,190,28,190,71,190,152,190,107,190,36,128,170,91,190,105,89,70,190,53,190,88,190,160,174,190,166,159,113,176,22,7,54,190,31,142,162,97,139,96,41,8,48,123,57,190,110,90,168,58,150,104,64,140,70,24,45,20,12,106,172,188,122,190,90,177,60,7,161,149,107,190,153,121,101,190,112,131,30,34,15,133,109,42,49,61,2,99,65,70,143,1,9,200,10,22,6,42,12,28,18,16,34,31,3,20,39,5,50,36,41,50,13,50,9,5,5,50,50,7,25,1,50,38,42,24,28,37,42,6,29,50,50,14,21,11,11,11,4,6,26,2,17,7,23,12,15,50,30,50,27,8,50,50,19,50,10,33,41,45]\n",
        "i=0\n",
        "#With a simple \"for loop\", we can add the whole list the breed_label dataframe\n",
        "for i in range (len(breeds_popularity_list)):\n",
        "        breeds.at[i,\"Ranked Popularity\"]= breeds_popularity_list[i]\n",
        "breeds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fc23b688b05286fa4f718108ada55dbd57a0a16e",
        "id": "wKMbRXKaTApZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "breeds_dictionary=breeds.set_index('BreedID')['Ranked Popularity'].to_dict()\n",
        "\n",
        "#We map the dictionary onto the train dataframe\n",
        "data[\"Breed1\"]=data[\"Breed1\"].map(breeds_dictionary)\n",
        "data[\"Breed2\"]=data[\"Breed2\"].map(breeds_dictionary)\n",
        "\n",
        "#if we  only have one breed for both columns, then we'll repeat the bread, indicating that this is a pure breed\n",
        "data[\"Breed2\"].fillna(data[\"Breed1\"],inplace=True)\n",
        "data[\"Breed1\"].fillna(data[\"Breed2\"],inplace=True)\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c2249df6a16c4597dff9575112e2aa7d73d24c4a",
        "id": "OhU4rt7eTApf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now we need to do the same with colors. At first I tried to use one hot encoding and create 7 new columns but that matrix was too sparse for my liking. So what I decided to do was assign a intensity value between 0 and 1 ( 0 being \"white\" and 1 being \"Black\") and mapped the colour value that way. The model seemed to perform better this way:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8edf78a2c4dab1e4160fdd378d844e2316729de4",
        "id": "JXyNa2zNTAph",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "colors = pd.read_csv('../input/color_labels.csv')\n",
        "\n",
        "\n",
        "colors.at[0, \"Color Intensity\"] = 1\n",
        "colors.at[1, \"Color Intensity\"] = 0.883\n",
        "colors.at[2, \"Color Intensity\"] = 0.667\n",
        "colors.at[3, \"Color Intensity\"] = 0.333\n",
        "colors.at[4, \"Color Intensity\"] = 0.167\n",
        "colors.at[5, \"Color Intensity\"] = 0.5\n",
        "colors.at[6, \"Color Intensity\"] = 0\n",
        "\n",
        "colors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c5e1ec4899d9abda7409cea731c963d8d3c6a50",
        "id": "-yKbjMyVTApn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "color_dictionary=colors.set_index('ColorID')['Color Intensity'].to_dict()\n",
        "\n",
        "\n",
        "data[\"Color2\"]= data[\"Color2\"].apply(lambda x: None if x==0 else x)\n",
        "data[\"Color3\"]= data[\"Color3\"].apply(lambda x: None if x==0 else x)\n",
        "\n",
        "\n",
        "data[\"Color1\"]=data[\"Color1\"].map(color_dictionary)\n",
        "data[\"Color2\"]=data[\"Color2\"].map(color_dictionary)\n",
        "data[\"Color3\"]=data[\"Color3\"].map(color_dictionary)\n",
        "\n",
        "#This always to standerize the colors, if color1=color2=color3, then the pet only has one solid color\n",
        "data[\"Color2\"].fillna(data[\"Color1\"],inplace=True)\n",
        "data[\"Color3\"].fillna(data[\"Color2\"],inplace=True)\n",
        "\n",
        "#for the second row (id=29ffe21fe), we can see that that the pet is black and white\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ee42afc09df61a48116432d87672b4b46932e33e",
        "id": "KR2w3ZVuTApt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2- Feature Standarization\n",
        "\n",
        "I'm only going to to use some pretty basic Label Encoding here..."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd8c757aac090bdddf9184311565ec5f2be683dc",
        "id": "3UB-IpoyTApv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data[\"Vaccinated\"]=data[\"Vaccinated\"].apply(lambda x:1 if x==1 else 0)\n",
        "data[\"Dewormed\"]=data[\"Dewormed\"].apply(lambda x:1 if x==1 else 0)\n",
        "data[\"Sterilized\"]=data[\"Sterilized\"].apply(lambda x:1 if x==1 else 0)\n",
        "#data[\"Gender\"]=data[\"Gender\"].apply(lambda x:1 if x==2 else 0)\n",
        "\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a62c2fcecbb9d98e9d78b9fce47c7084f95fcf48",
        "id": "1CuLDL5mTAp1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's just save our formatted data just in case.\n",
        "\n",
        "data.to_csv(\"formated_data_petfinder.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0d5c62836ef7561ff2353b8480ed9c2f68b12063",
        "id": "ZjUM1iAaTAp-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3- Feature Selection\n",
        "\n",
        "So let's start doing so analysis on our data. First, let's check what the most important features seem to be:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "72ab8396ed52f011363bb149b449213e3bab16c7",
        "id": "IIpr_eHrTAqE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating our x datasheet, which contains all our data except for the ['AdoptionSpeed'] column.\n",
        "x=data.drop(columns='AdoptionSpeed')\n",
        "\n",
        "\n",
        "#The ['AdoptionSpeed'] column extracted.\n",
        "y=data['AdoptionSpeed']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8c1016de9c787eb39db3503894e42d4fea69c0ca",
        "id": "84bq9KN1TAqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Checking the features that are correlated to the target feature (Adoption speed)\n",
        "for columnname in x.columns: \n",
        "    if abs(x[columnname].corr(y)) >0.095 :    \n",
        "      print('The Correlation Between',columnname,'and Adoption Speed is significant:', abs(x[columnname].corr(y)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "62bbddd9644856e1d4d41770231292b12f5b6225",
        "id": "xqLCTjcJTAqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "From this we can learn that the Breed, Age, Vaccination and Sterilization seem to be key predictors ( there might be others of course.)\n",
        "\n",
        "Let's now use a Decision Tree to keeping viewing the Feature Importance."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10e61954ac42d2d0b78bb117051c44052d8901c8",
        "id": "CGojrkVFTAql",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "feature_importance_indicator=ExtraTreesClassifier(n_estimators = 100)\n",
        "\n",
        "#Then we ask this algorithm, now dubbed as \"feature_importance_indicator\", to learn from our data x ( the indicators) and y( the Adoption Speed outcome):\n",
        "\n",
        "feature_importance_indicator.fit(x,y)\n",
        "\n",
        "#We then ask the model politely to create a list of which columns it learnt the most from and which columns didn't help it at all, for this we use the \"feature_importances_\" command:\n",
        "\n",
        "importance_dict= dict(zip(x.columns, (feature_importance_indicator.feature_importances_)))\n",
        "\n",
        "#We sort the list in descending order so that it is quicker to analyse.\n",
        "\n",
        "importance_dict_sorted= OrderedDict(sorted(importance_dict.items(), key=lambda x:x[1],reverse=1))\n",
        "\n",
        "#The \"pprint\" commnad allows us to print things out nicely.\n",
        "\n",
        "pprint.pprint(importance_dict_sorted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4002ddad3b39943b4738155a4f213aba85a14c3",
        "id": "hb8ZmHITTAqr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = x.columns\n",
        "importances = feature_importance_indicator.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], color='r', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.xlabel('Importance Level')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c45179419c4358a9069ff0c7041861a59eeb11cd",
        "id": "dihJwODITAqw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After analysing the resukts, I reckon we can drop the following two features as they seem to be pretty irrevelant:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfb8298b1a21ab6f4db67f12763e111b56118528",
        "id": "ksI-b71jTAqy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data.drop(columns=[\"Gender\",\"VideoAmt\"],inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3b633c68af4400782e47d2c275afb0b5d5f81795",
        "id": "QJXm39KFTAq3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##For the key part of this kernel!!:\n",
        "\n",
        "Now, in my mind there was something that was bugging me about the dataset, and that was the fact that dataset  was including dogs and cats together. Now I know a thing or two about pet adoption, and for me it is pretty obvious that the criteria a person uses when accepting a cat is rather different to that of adopting a dog. People simply take different things into account. Ultimately, different features would have different importance based whether it was a cat or a dog. \n",
        "Because i'm using some very narrow M.L here (just SciKit Learn Algorithms) was afraid that this distinction wouldn't be weighted strong enough in the model that tries to predict for both Cats and Dogs. \n",
        "\n",
        "So I decided to visualize the data to see how significant the difference was between cat and dog feature, to see if I could validate by theory or not. "
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f929842110eaeda0346c8ad280a64e086358886",
        "id": "bAMAG-UnTAq6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_correlation_matrix(data[data.Type==1].drop(columns=[\"Type\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b8d3b931d73b64fcee5ed37cdea6651b605b211",
        "id": "StremV3DTArE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plot_correlation_matrix(data[data.Type==2].drop(columns=[\"Type\"]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cc97a7e8232035a4b471017365270183e0c783ff",
        "id": "bB6n1kDkTArJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rad_viz = pd.plotting.radviz(x, 'Type')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b145cf9583095ab6ea475e1b80c665e713a24c08",
        "id": "WIX1UAAyTArQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In my opinion, the Correlation Matrixes show us that the two datasets are rather different indeed, different features seem to be correlated differently to one another depending on the Animal type.\n",
        "Futhermore, if we have take a look at the Adoption Speed line, we can see some difference in the correlations between the two the matrixes.\n",
        "Finally, the Radviz allows us to see that the cat data is a lot more spread out whereas the dog data is far more uniform. \n",
        "\n",
        "Therefore, I will try to seperate this dataset into two subdatasets ( one of cats, one of dogs) and train two different models on them."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8ba8faff5255e0ecc45911986c85daa9af9f959e",
        "id": "jUQIYd2TTArS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#we create two datasets, one for dogs and one for cats to train seperate data on them.\n",
        "\n",
        "dogdata = data[data.Type==1]\n",
        "catdata = data[data.Type==2]\n",
        "\n",
        "dogdata.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "847384640a4b5f9245e996d38f338b883ce4b7c7",
        "id": "ObRcCYTxTAra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Part 2- Data Learning\n",
        "\n",
        "We'll now train our two datasets on different models. To find the optimal model for each dataset, I used the highly recommendable[ TPOT api](https://epistasislab.github.io/tpot/api/). TPOT uses genetic searching to give you the Sci kit learn model with the optimal hyperparameters. However, this process took roughly 8 hours overall. The models and hyperparameters used here below here the optimal models that the TPOT alogorithm found."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1942c28a64beb86ddefca6ac04e92a6789616869",
        "id": "QEffykVQTArc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "dog_x=dogdata.drop(columns=[\"AdoptionSpeed\"])\n",
        "dog_y=dogdata[\"AdoptionSpeed\"]\n",
        "#splitting between train and test for the dog data\n",
        "dog_x_training,dog_x_testing, dog_y_training, dog_y_testing = train_test_split(dog_x, dog_y, test_size=0.1, random_state=85)\n",
        "\n",
        "#splitting between train and test for the cat data\n",
        "\n",
        "cat_x=catdata.drop(columns=[\"AdoptionSpeed\"])\n",
        "cat_y=catdata[\"AdoptionSpeed\"]\n",
        "\n",
        "cat_x_training,cat_x_testing,  cat_y_training, cat_y_testing = train_test_split(cat_x, cat_y, test_size=0.1, random_state=85)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dd3b2eb14b520d67b6db779c00217aed610b64d1",
        "id": "oRc94U6RTArj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here is the best model found for the dogdata, GradientBoosting Classifier:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08143d669e61b733d7aa64c18876fe9be8ae17b9",
        "id": "XnZNXWUeTArl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "\n",
        "# Average CV score on the training set was:0.368919785590269\n",
        "\n",
        "dog_best_model = make_pipeline(\n",
        "    VarianceThreshold(threshold=0.0001),\n",
        "    GradientBoostingClassifier(learning_rate=0.1, max_depth=5, max_features=0.5, min_samples_leaf=4, min_samples_split=17, n_estimators=100, subsample=1.0)\n",
        ")\n",
        "\n",
        "\n",
        "dog_best_model.fit(dog_x_training, dog_y_training)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dea913d6738b5ca55d5d026aeeb8a8522ec5d74e",
        "id": "cvNe-AjHTArq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can use our Quadratic Weighted Kappa (QWK) function from earlier to test it on our test data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbbb60778839211b68678cbc5aada35d9358feb4",
        "id": "EWbf4LjrTArs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_true_train= np.array(dog_y_training)\n",
        "y_pred_train = np.array(dog_best_model.predict(dog_x_training))\n",
        "\n",
        "y_true_test= np.array(dog_y_testing)\n",
        "y_pred_test = np.array(dog_best_model.predict(dog_x_testing))\n",
        "\n",
        "\n",
        "print(\"On training data QWK:...\",quadratic_kappa(y_true_train,y_pred_train),\"on testing data QWK...\",quadratic_kappa(y_true_test,y_pred_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d61c80c56647d3b27a08811eb3880c64e85c3171",
        "id": "75t7CW7rTAr3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now for the cat data. Here the best model was found to be a Scaler along with XGBoost. The fact that the optimal models found for each dataset reinforces by point of view that cat data and dog data should be treated differently."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1126617d733cd9563c74a4c1f59d69fc14d9f33b",
        "id": "20yTHrOZTAr5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# Average CV score on the training set was:0.31768177281003007\n",
        "cat_best_model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    XGBClassifier(learning_rate=0.1, max_depth=4, min_child_weight=13, n_estimators=100, nthread=1, subsample=0.7500000000000001)\n",
        ")\n",
        "\n",
        "cat_best_model.fit(cat_x_training,cat_y_training)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "959120d17582327fd3adf27683c27bf0ccf251c6",
        "id": "L2Fu3cuDTAsB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_true_train= np.array(cat_y_training)\n",
        "y_pred_train = np.array(cat_best_model.predict(cat_x_training))\n",
        "\n",
        "y_true_test= np.array(cat_y_testing)\n",
        "y_pred_test = np.array(cat_best_model.predict(cat_x_testing))\n",
        "\n",
        "print(\"On training data...\",quadratic_kappa(y_true_train,y_pred_train),\"on testing data...\",quadratic_kappa(y_true_test,y_pred_test))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bfb9bbadf2b5546321f2e4173b4f02b08086bd0d",
        "id": "T7dl0aosTAsG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "On a final note, I also ran TPOT on the whole data (cats on dogs put together), here the best QWK I was able to get on the test data was 0.31...  In my opinion, seperating the catdata from the dogdata is the right way to go in order to increase the performance of our models. I encourage others to try it with more complex models that use the image dataset and NLP."
      ]
    },
    {
      "metadata": {
        "_uuid": "1d065d65b997e5b63a97f0e0235d88d4965a90b3",
        "id": "7JacFqLBTAsI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Part 3- Data Predicting\n",
        "\n",
        "Let's know use these two models to generate our answers for the submision:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4af90b4d848e5ba7e01ce97542d8a1f9ee77f37",
        "id": "QVUQzVOtTAsJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let's first retrain our models on the complete dataset\n",
        "dog_best_model.fit(dog_x, dog_y)\n",
        "cat_best_model.fit(cat_x,cat_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4520032c0ce14b9f0928d48ccfd90ec7ea33e129",
        "id": "9v6BOZ--TAsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading the test data provided by Kaggle\n",
        "test_data=pd.read_csv(\"../input/test/test.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dc7a42cd681f14cc98e357c9d79190808df43086",
        "id": "XgwLSSvaTAsT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Formatting the test data just like we did with the train data...\n",
        "test_data.drop(columns=['RescuerID','Name','Description','PetID','Gender','VideoAmt'], inplace=True)\n",
        "\n",
        "test_data[\"State urbanisation\"]=test_data[\"State\"].map(states_ubran_dict)\n",
        "test_data[\"State population\"]=test_data[\"State\"].map(states_pop_dict)\n",
        "test_data.drop(columns=\"State\", inplace= True)\n",
        "\n",
        "test_data[\"Color2\"]= test_data[\"Color2\"].apply(lambda x: None if x==0 else x)\n",
        "test_data[\"Color3\"]= test_data[\"Color3\"].apply(lambda x: None if x==0 else x)\n",
        "\n",
        "\n",
        "test_data[\"Color1\"]=test_data[\"Color1\"].map(color_dictionary)\n",
        "test_data[\"Color2\"]=test_data[\"Color2\"].map(color_dictionary)\n",
        "test_data[\"Color3\"]=test_data[\"Color3\"].map(color_dictionary)\n",
        "\n",
        "test_data[\"Color2\"].fillna(test_data[\"Color1\"],inplace=True)\n",
        "test_data[\"Color3\"].fillna(test_data[\"Color2\"],inplace=True)\n",
        "\n",
        "test_data[\"Breed1\"]=test_data[\"Breed1\"].map(breeds_dictionary)\n",
        "test_data[\"Breed2\"]=test_data[\"Breed2\"].map(breeds_dictionary)\n",
        "\n",
        "test_data[\"Breed2\"].fillna(test_data[\"Breed1\"],inplace=True)\n",
        "test_data[\"Breed1\"].fillna(test_data[\"Breed2\"],inplace=True)\n",
        "\n",
        "test_data[\"Vaccinated\"]=test_data[\"Vaccinated\"].apply(lambda x:1 if x==1 else 0)\n",
        "test_data[\"Dewormed\"]=test_data[\"Dewormed\"].apply(lambda x:1 if x==1 else 0)\n",
        "test_data[\"Sterilized\"]=test_data[\"Sterilized\"].apply(lambda x:1 if x==1 else 0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "test_data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c81bc4b5a553625c0d59385fc1e672e4287ebe2b",
        "id": "EFGyGIJITAsZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Making sure the formatting was done correctly\n",
        "if len(test_data.columns)==len(dog_x.columns):\n",
        "  print(\"Formatting done correctly\")\n",
        "else:\n",
        "    print(\"H we have a problem, double check previous cell\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd2401494ae229bf30decf0cc2db78d9282ab8c8",
        "id": "HA8q_aEuTAse",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to apply the models correspondingly, I built the function below that discriminates between dogs and cats in the Test data and uses the correct model on that specific row."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf7a5d270dc2643283f5899d71688060f645d48c",
        "id": "oJ7USGpWTAsg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We'll store our answers in this dictionary so that we can map them onto the submission file\n",
        "answers_dictionary={}\n",
        "\n",
        "def test_predicitions(test_data):\n",
        "    assert type(test_data)==type(data)\n",
        "    for i in range (len(test_data)):\n",
        "        if int(test_data[\"Type\"].iloc[[i,]])==1:\n",
        "            test_data.iloc[[i,]].drop(columns=\"Type\")\n",
        "            answers_dictionary[test_data.index[i]]=int(dog_best_model.predict(test_data.iloc[[i,]]))\n",
        "        else:\n",
        "            test_data.iloc[[i,]].drop(columns=\"Type\")\n",
        "            answers_dictionary[test_data.index[i]]=int(cat_best_model.predict(test_data.iloc[[i,]]))\n",
        "\n",
        "test_predicitions(test_data)     \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34387ca3064401f203a2dbc14556c332fffc6d61",
        "id": "vnS-CydlTAso",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission_answer=pd.read_csv(\"../input/test/sample_submission.csv\")\n",
        "\n",
        "submission_answer.head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7e53c003c5205b91d8339d70073132c1a2c27e9",
        "id": "z9DNbNDfTAs3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#we map our asnwers from our two different models onto the submission file\n",
        "submission_answer[\"AdoptionSpeed\"]= submission_answer.index.map(answers_dictionary.get)\n",
        "\n",
        "submission_answer.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3acddae4deb73d82ea151ae6f527c235d5a2f873",
        "id": "1DhILxD5TAs9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission_answer.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eb4bd1426640f53aca716020311f1438c184c076",
        "id": "mEJ1RfIsTAtG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "My conclusion is that I think this problem needs to be treated as one with two independent sub-sets of data. From my own experience, when I seperated the cat of the dogs, my models, as simple as they were, improved significantly. I am therefore exicted to see if others take this approach and can get even better results by seperating the two datasrts with the use of DL, Computer vision and Sentiment Analysis."
      ]
    }
  ]
}